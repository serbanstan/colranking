{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(1)\n",
    "\n",
    "# number of users\n",
    "d1 = 10;\n",
    "\n",
    "# number of items\n",
    "d2 = 13;\n",
    "\n",
    "# number of queries\n",
    "n = 300;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the hidden variables as a 1-rank\n",
    "ThetaS = rand(d1) * rand(d2)'\n",
    "\n",
    "# Need to have the sum of rows equaling 0\n",
    "for i = 1:d1\n",
    "    ThetaS[i,:] -= mean(ThetaS[i,:])\n",
    "end\n",
    "\n",
    "# Need to make the Frobenius norm < 1\n",
    "ThetaS = ThetaS / vecnorm(ThetaS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for i = 1:n\n",
    "    Xi = zeros(d1, d2)\n",
    "    \n",
    "    lin = rand(1:d1)\n",
    "    c1 = rand(1:d2)\n",
    "    c2 = rand(1:d2)\n",
    "    while c2 == c1\n",
    "        c2 = rand(1:d2)\n",
    "    end\n",
    "    \n",
    "    Xi[lin,c1] = 1\n",
    "    Xi[lin,c2] = -1\n",
    "    Xi = Xi * sqrt(d1 * d2)\n",
    "    \n",
    "    push!(X, Xi)\n",
    "    \n",
    "    if ThetaS[lin,c1] > ThetaS[lin,c2]\n",
    "        push!(y, 1)\n",
    "    else\n",
    "        push!(y, 0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nucNorm (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nucNorm(A)\n",
    "    return sum(svd(A)[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjust (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the matrix is centered and with norm < 1\n",
    "\n",
    "function adjust(X)\n",
    "    ans = copy(X)\n",
    "    \n",
    "    for i = 1:d1\n",
    "        ans[i,:] -= mean(ans[i,:])\n",
    "    end\n",
    "    \n",
    "    return ans / vecnorm(ans)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loss function\n",
    "\n",
    "function loss(Theta)\n",
    "    ans = 0\n",
    "    \n",
    "    for i = 1:n\n",
    "        ans = ans + log(1 + e^(trace(Theta' * X[i]))) - y[i] * trace(Theta' * X[i])\n",
    "    end\n",
    "    \n",
    "    return ans / n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deltaF (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the derivative of the loss function. This will be the 'v' in the proximal step\n",
    "\n",
    "function deltaF(Theta)\n",
    "    \n",
    "    ans = zeros(d1,d2)\n",
    "    \n",
    "    for i = 1:n\n",
    "        t1 = 1 / (1 + e^trace(Theta' * X[i]))\n",
    "        t2 = e^trace(Theta' * X[i]) * X[i]\n",
    "        t3 = y[i] * X[i]\n",
    "        \n",
    "        ans = ans + t1 * t2 - t3\n",
    "    end\n",
    "    \n",
    "    return ans / n\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proxObj (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The proximal objective\n",
    "\n",
    "function proxObj(lambda, X, V)\n",
    "    return nucNorm(X) + 1 / (2 * lambda) * vecnorm(X - V)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prox (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The proximal function\n",
    "# return the matrix X that minimizes lambda * ||X||_nuc + 1/(2 lambda) || X - V ||_2^2\n",
    "\n",
    "function prox(lambda, V)\n",
    "    \n",
    "    X = adjust(rand(d1,d2))\n",
    "    Xmin = copy(X)\n",
    "    \n",
    "    step = 0.05\n",
    "    eps = 0.2\n",
    "    numSteps = 0\n",
    "    \n",
    "    while numSteps < 200\n",
    "        numSteps += 1\n",
    "    \n",
    "        # a subgradient of lambda * ||X||_nuc is AB' where X = ASB'\n",
    "        A,_,B = svd(X)\n",
    "        t1 = lambda * A * B'\n",
    "\n",
    "        # the derivative of 1 / (2 lambda) || X - V ||_2^2 is 1/lambda * (X - V)\n",
    "        t2 = 1 / lambda * (X - V)\n",
    "        \n",
    "        newX = X - step * (t1 + t2)\n",
    "        newX = adjust(newX)\n",
    "        \n",
    "        if proxObj(lambda, X, V) < eps\n",
    "            return X\n",
    "        end\n",
    "        \n",
    "        if proxObj(lambda, X, V) < proxObj(lambda, Xmin, V)\n",
    "            Xmin = copy(X)\n",
    "        end\n",
    "        \n",
    "        X = copy(newX)\n",
    "    end\n",
    "    \n",
    "    return Xmin\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a sanity check, make sure that our gradient steps indeed decrease loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden loss 0.39587472868210566\n",
      "Current loss 0.7871777566811669\n",
      "Loss after 1 steps: 0.7549552776556652\n",
      "Loss after 2 steps: 0.7240941139300339\n",
      "Loss after 3 steps: 0.6946391989696824\n",
      "Loss after 4 steps: 0.666617723568723\n",
      "Loss after 5 steps: 0.6400403102197109\n",
      "Loss after 6 steps: 0.6149024316085975\n",
      "Loss after 7 steps: 0.5911860132867398\n",
      "Loss after 8 steps: 0.5688611687102807\n",
      "Loss after 9 steps: 0.5478880188733919\n",
      "Loss after 10 steps: 0.5282185458094266\n"
     ]
    }
   ],
   "source": [
    "Theta = adjust(rand(d1,d2))\n",
    "\n",
    "println(\"Hidden loss \", loss(ThetaS))\n",
    "println(\"Current loss \", loss(Theta))\n",
    "\n",
    "for stp = 1:10\n",
    "    alpha = 0.05\n",
    "    der = deltaF(Theta)\n",
    "\n",
    "    newTheta = Theta - alpha * der\n",
    "    newTheta = adjust(newTheta)\n",
    "\n",
    "    println(\"Loss after \", stp, \" steps: \", loss(newTheta))\n",
    "    \n",
    "    Theta = copy(newTheta)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's test out the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f + g = 0.681113129753132\n",
      "f + g = 0.4749330301788126\n",
      "f + g = 0.4107402726800853\n",
      "f + g = 0.38946601515297535\n",
      "f + g = 0.382164842661707\n",
      "f + g = 0.3795753705696425\n",
      "f + g = 0.37875050542011324\n",
      "f + g = 0.3782484797098058\n",
      "f + g = 0.3780198231363411\n",
      "f + g = 0.37764489218423236\n"
     ]
    }
   ],
   "source": [
    "curX = adjust(rand(d1,d2))\n",
    "lambda = 0.1\n",
    "\n",
    "for steps = 1:100\n",
    "    # first, go in the direction of F\n",
    "    interX = curX - lambda * deltaF(curX)\n",
    "    newX = prox(lambda, interX)\n",
    "    \n",
    "    if steps % 10 == 0\n",
    "        println(\"f + g = \", loss(curX) + lambda * nucNorm(curX))\n",
    "    #     println(\"f = \", loss(curX))\n",
    "    #     println(\"g = \", lambda * nucNorm(curX))\n",
    "    #     println()\n",
    "    end\n",
    "    \n",
    "    curX = copy(newX)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4958747286821057\n"
     ]
    }
   ],
   "source": [
    "println(loss(ThetaS) + lambda * nucNorm(ThetaS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
