{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(1)\n",
    "\n",
    "# number of users\n",
    "d1 = 190;\n",
    "\n",
    "# number of items\n",
    "d2 = 210;\n",
    "\n",
    "# number of queries\n",
    "n = 6000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the hidden ratings\n",
    "ThetaS = rand(d1,4) * rand(4,d2)\n",
    "\n",
    "# Need to have the sum of rows equaling 0\n",
    "for i = 1:d1\n",
    "    ThetaS[i,:] -= mean(ThetaS[i,:])\n",
    "end\n",
    "\n",
    "# Need to make the Frobenius norm <= 1\n",
    "ThetaS = ThetaS / vecnorm(ThetaS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for i = 1:n\n",
    "    Xi = zeros(d1, d2)\n",
    "    \n",
    "    lin = rand(1:d1)\n",
    "    c1 = rand(1:d2)\n",
    "    c2 = rand(1:d2)\n",
    "    while c2 == c1\n",
    "        c2 = rand(1:d2)\n",
    "    end\n",
    "    \n",
    "    Xi[lin,c1] = 1\n",
    "    Xi[lin,c2] = -1\n",
    "    Xi = Xi * sqrt(d1 * d2)\n",
    "    \n",
    "    push!(X, Xi)\n",
    "    \n",
    "    if ThetaS[lin,c1] > ThetaS[lin,c2]\n",
    "        push!(y, 1)\n",
    "    else\n",
    "        push!(y, 0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nucNorm (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nucNorm(A)\n",
    "    return sum(svd(A)[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjust (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the matrix's rows are centered\n",
    "\n",
    "function adjust(X)\n",
    "    ans = copy(X)\n",
    "    \n",
    "    for i = 1:size(X)[1]\n",
    "        ans[i,:] -= mean(ans[i,:])\n",
    "    end\n",
    "    \n",
    "    return ans\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The objective function\n",
    "\n",
    "function obj(Theta, ns)\n",
    "    ans = 0\n",
    "    \n",
    "    for i = 1:ns\n",
    "        ans = ans + log(1 + e^(trace(Theta' * X[i]))) - y[i] * trace(Theta' * X[i])\n",
    "    end\n",
    "    \n",
    "    return ans / ns\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deltaF (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the derivative of the objective loss\n",
    "# Theta is our current guess\n",
    "# numSamples represents the number of X's we are considering\n",
    "\n",
    "function deltaF(Theta, ns)\n",
    "    \n",
    "    ans = zeros(d1,d2)\n",
    "    \n",
    "    for i = 1:ns\n",
    "        t = e^trace(Theta' * X[i])\n",
    "        ans = ans + (t / (1 + t) - y[i]) * X[i]\n",
    "    end\n",
    "    \n",
    "    return ans / ns\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prox (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The proximal function\n",
    "# return the matrix X that minimizes lambda * ||X||_nuc + 1/(2 lambda_k) || X - V ||_F^2\n",
    "# equivalent to (lamda*lambda_k) ||X||_nuc + 1/2 || X - V ||_F^2\n",
    "\n",
    "function prox(lambda, V)\n",
    "    \n",
    "    A,S,B = svd(V)\n",
    "    S = max(S - lambda, 0)\n",
    "    \n",
    "    return adjust(A * diagm(S) * B')\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkKKT (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function checkKKT(lamb, Theta, ns)\n",
    "    println(\"Checking KKT conditions\")\n",
    "    \n",
    "    tot = 0\n",
    "    \n",
    "    for i = 1:ns\n",
    "        tot = tot + (e ^ trace(Theta' * X[i]) / (1 + e ^ trace(Theta' * X[i])) - y[i]) * X[i]\n",
    "    end\n",
    "    \n",
    "    tot = tot / ns\n",
    "    \n",
    "    U,S,V = svd(Theta)\n",
    "    k = rank(Theta)\n",
    "    U = U[:,1:k]\n",
    "    V = V[:,1:k]\n",
    "    \n",
    "    tot = tot + lamb * U * V'\n",
    "    \n",
    "    if norm(tot) <= lambda\n",
    "        println(\"GOOD - the norm is smaller than lambda\")\n",
    "    else\n",
    "        println(\"OOPS - the norm is greater than lambda\")\n",
    "    end\n",
    "    \n",
    "    println(norm(U' * tot))\n",
    "    \n",
    "    println(norm(tot * V))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a sanity check, make sure that our gradient steps indeed decreases obj loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden loss 0.33953828241367656\n",
      "Current loss 33.52888351973209\n",
      "Loss after 1 steps: 26.581566144922824\n",
      "Loss after 2 steps: 20.740166384947962\n",
      "Loss after 3 steps: 15.91524159581662\n",
      "Loss after 4 steps: 11.989627851306716\n",
      "Loss after 5 steps: 8.842892554875345\n"
     ]
    }
   ],
   "source": [
    "Theta = adjust(rand(d1,d2))\n",
    "\n",
    "println(\"Hidden loss \", obj(ThetaS, n))\n",
    "println(\"Current loss \", obj(Theta, n))\n",
    "\n",
    "for stp = 1:5\n",
    "    alpha = 1.0\n",
    "\n",
    "    newTheta = Theta - alpha * deltaF(Theta, n)\n",
    "    newTheta = adjust(newTheta)\n",
    "\n",
    "    println(\"Loss after \", stp, \" steps: \", obj(newTheta, n))\n",
    "    \n",
    "    Theta = copy(newTheta)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll pick a specific number of samples in terms of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numSamples = []\n",
    "for den = [3,2,1.5,1.33,1.16,1]\n",
    "    push!(numSamples, convert(Int64, floor(n / den)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's test out the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with 2000 samples\n",
      "at iteration 25\n",
      "at iteration 50\n",
      "at iteration 75\n",
      "at iteration 100\n",
      "at iteration 125\n",
      "at iteration 150\n",
      "at iteration 175\n",
      "at iteration 200\n",
      "f + g = 0.3599593161503586\n",
      "f = 0.13859918383959072\n",
      "g = 0.22136013231076787\n",
      "Loss = 0.6302220578409478\n",
      "\n",
      "Checking KKT conditions\n",
      "GOOD - the norm is smaller than lambda\n",
      "0.026006643063403678\n",
      "0.026081067265269528\n",
      "\n",
      "335.078655 seconds (6.62 M allocations: 375.950 GB, 11.76% gc time)\n",
      "\n",
      "\n",
      "working with 3000 samples\n",
      "at iteration 25\n",
      "at iteration 50\n",
      "at iteration 75\n",
      "at iteration 100\n",
      "at iteration 125\n",
      "at iteration 150\n",
      "at iteration 175\n",
      "at iteration 200\n",
      "f + g = 0.3923828347903726\n",
      "f = 0.1682680850144779\n",
      "g = 0.22411474977589468\n",
      "Loss = 0.41860726100589996\n",
      "\n",
      "Checking KKT conditions\n",
      "GOOD - the norm is smaller than lambda\n",
      "0.01856922766887109\n",
      "0.018597279727144592\n",
      "\n",
      "498.430389 seconds (9.66 M allocations: 563.232 GB, 11.39% gc time)\n",
      "\n",
      "\n",
      "working with 4000 samples\n",
      "at iteration 25\n",
      "at iteration 50\n",
      "at iteration 75\n",
      "at iteration 100\n",
      "at iteration 125\n",
      "at iteration 150\n",
      "at iteration 175\n",
      "at iteration 200\n",
      "f + g = 0.41069331825563915\n",
      "f = 0.1894890324633335\n",
      "g = 0.2212042857923056\n",
      "Loss = 0.3566907815727066\n",
      "\n",
      "Checking KKT conditions\n",
      "GOOD - the norm is smaller than lambda\n",
      "0.010763356000390776\n",
      "0.010835273263485034\n",
      "\n",
      "678.118941 seconds (12.92 M allocations: 750.523 GB, 10.68% gc time)\n",
      "\n",
      "\n",
      "working with 4511 samples\n",
      "at iteration 25\n",
      "at iteration 50\n",
      "at iteration 75\n",
      "at iteration 100\n",
      "at iteration 125\n",
      "at iteration 150\n",
      "at iteration 175\n",
      "at iteration 200\n",
      "f + g = 0.4185159179602942\n",
      "f = 0.1979915444963814\n",
      "g = 0.22052437346391282\n",
      "Loss = 0.34940502656072336\n",
      "\n",
      "Checking KKT conditions\n"
     ]
    }
   ],
   "source": [
    "curTheta = zeros(d1,d2) \n",
    "lambda = 0.1   # the regularization parameter\n",
    "lambda_k = .05   # the step size\n",
    "\n",
    "overallLF = []\n",
    "\n",
    "for ns = numSamples\n",
    "    LF = []\n",
    "    \n",
    "    println(\"working with \", ns, \" samples\")\n",
    "    \n",
    "    maxit = 200\n",
    "    @time for steps = 1:maxit\n",
    "        # retain the old loss\n",
    "        oldLoss = vecnorm(ThetaS - curTheta)^2\n",
    "        \n",
    "        # first, go in the direction of f\n",
    "        V = curTheta - lambda_k * deltaF(curTheta, ns)\n",
    "        \n",
    "#         println(maximum(V), \" \", minimum(V))\n",
    "#         if steps > 350\n",
    "#             println(V)\n",
    "#         end\n",
    "        \n",
    "        # now, perform the proximal step\n",
    "        newTheta = prox(lambda * lambda_k, V)\n",
    "\n",
    "        # some information about runs\n",
    "        if steps % 25 == 0 || steps == maxit\n",
    "            println(\"at iteration \", steps)\n",
    "            \n",
    "            if steps == maxit\n",
    "                println(\"f + g = \", obj(curTheta, ns) + lambda * nucNorm(curTheta))\n",
    "                println(\"f = \", obj(curTheta, ns))\n",
    "                println(\"g = \", lambda * nucNorm(curTheta))\n",
    "                println(\"Loss = \", vecnorm(ThetaS - newTheta)^2)\n",
    "                println()\n",
    "                checkKKT(lambda, newTheta, ns)\n",
    "                println()\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # update the loss array\n",
    "        push!(LF, vecnorm(ThetaS - newTheta)^2)\n",
    "        \n",
    "        # check if the loss is not decreasing anymore\n",
    "        newLoss = vecnorm(ThetaS - newTheta)^2\n",
    "#         if abs(oldLoss - newLoss) < 1e-7\n",
    "#         if newLoss < 0.4\n",
    "#             push!(overallLF, vecnorm(ThetaS - newTheta)^2)\n",
    "            \n",
    "#             println(\"Converged after \", steps, \" iterations\")\n",
    "#             checkKKT(lambda, newTheta, ns)\n",
    "#             println()\n",
    "            \n",
    "#             break\n",
    "#         end\n",
    "        \n",
    "        # if I hit the max number of steps, stop anyway\n",
    "        if steps == maxit\n",
    "            push!(overallLF, vecnorm(ThetaS - newTheta)^2)\n",
    "        end\n",
    "        \n",
    "        # update Theta for the next iteration\n",
    "        curTheta = copy(newTheta)\n",
    "    end\n",
    "    \n",
    "    dims = convert(Int64, (d1+d2)/2)\n",
    "    filename = string(\"./data/\", dims, \"-\", ns, \".txt\")\n",
    "    writedlm(filename, LF)\n",
    "    \n",
    "    println()\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writedlm(string(\"./data/\", convert(Int64, (d1+d2)/2), \"-final.txt\"), overallLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the loss function\n",
    "plot(overallLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# newLF = []\n",
    "# for i = 1:length(LF)\n",
    "#     push!(newLF, log(LF[i]))\n",
    "# end\n",
    "\n",
    "# plot(newLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# writedlm(\"./data/t.txt\", LF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overallLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
